#!/usr/bin/env python3
"""
bbc-news-pipeline-70 pipeline

Streaming Dataflow pipeline for real-time data processing.
Generated by CI/CD Agent.
"""

import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions
from apache_beam.io.gcp.bigquery import ReadFromBigQuery, WriteToBigQuery
from apache_beam.transforms.window import FixedWindows
import argparse
import logging


class BbcNewsPipeline70Options(PipelineOptions):
    """Custom pipeline options for bbc-news-pipeline-70"""

    @classmethod
    def _add_argparse_args(cls, parser):
        parser.add_argument(
            '--input_table',
            required=True,
            help='Input BigQuery table in format PROJECT:DATASET.TABLE'
        )
        parser.add_argument(
            '--output_table',
            required=True,
            help='Output BigQuery table in format PROJECT:DATASET.TABLE'
        )
        parser.add_argument(
            '--project',
            required=True,
            help='GCP Project ID'
        )
        parser.add_argument(
            '--window_duration',
            type=int,
            default=60,
            help='Window duration in seconds'
        )


class ProcessRecord(beam.DoFn):
    """
    Process individual records in the streaming pipeline.

    This is where you add your custom business logic.
    """

    def process(self, element):
        """
        Process a single record.

        Args:
            element: Input record from BigQuery

        Yields:
            Processed record
        """
        try:
            # TODO: Add your custom transformation logic here
            # Example: Add timestamp, enrich data, filter, etc.

            processed = {
                **element,
                'processed_at': beam.pvalue.AsSingleton(
                    beam.Pipeline() | beam.Create([beam.window.TimestampedValue(None, 0)])
                )
            }

            yield processed

        except Exception as e:
            logging.error(f"Error processing record: {e}")
            # Optionally yield to dead letter queue


class AggregateRecords(beam.CombineFn):
    """
    Aggregate records within a window.

    Customize this to perform your aggregation logic.
    """

    def create_accumulator(self):
        return {'count': 0, 'records': []}

    def add_input(self, accumulator, element):
        accumulator['count'] += 1
        accumulator['records'].append(element)
        return accumulator

    def merge_accumulators(self, accumulators):
        merged = {'count': 0, 'records': []}
        for acc in accumulators:
            merged['count'] += acc['count']
            merged['records'].extend(acc['records'])
        return merged

    def extract_output(self, accumulator):
        return {
            'count': accumulator['count'],
            'window_data': accumulator['records']
        }


def run(argv=None):
    """
    Main entry point for the bbc-news-pipeline-70 streaming pipeline.
    """
    parser = argparse.ArgumentParser()
    known_args, pipeline_args = parser.parse_known_args(argv)

    # Pipeline options
    pipeline_options = PipelineOptions(pipeline_args)

    # Enable streaming mode
    pipeline_options.view_as(StandardOptions).streaming = True

    custom_options = pipeline_options.view_as(BbcNewsPipeline70Options)

    # Construct full table references
    input_table = f"{custom_options.project}:bbc_news.bbc_news"
    output_table = f"{custom_options.project}:bbc_news.processed_bbc_news"

    with beam.Pipeline(options=pipeline_options) as pipeline:
        # Read from BigQuery (streaming)
        raw_data = (
            pipeline
            | 'ReadFromBigQuery' >> ReadFromBigQuery(
                query=f'''
                    SELECT *
                    FROM `{input_table}`
                    WHERE _PARTITIONTIME >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)
                ''',
                use_standard_sql=True
            )
        )

        # Apply windowing
        windowed_data = (
            raw_data
            | 'ApplyWindowing' >> beam.WindowInto(
                FixedWindows(custom_options.window_duration)
            )
        )

        # Process records
        processed = (
            windowed_data
            | 'ProcessRecords' >> beam.ParDo(ProcessRecord())
        )

        # Optional: Aggregate within windows
        # aggregated = (
        #     processed
        #     | 'GroupByKey' >> beam.GroupBy(lambda x: x.get('key_field'))
        #     | 'Aggregate' >> beam.CombinePerKey(AggregateRecords())
        # )

        # Write to BigQuery
        (
            processed
            | 'WriteToBigQuery' >> WriteToBigQuery(
                output_table,
                schema='SCHEMA_AUTODETECT',
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,
                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
                method='STREAMING_INSERTS'  # Use streaming inserts for real-time
            )
        )


if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    run()