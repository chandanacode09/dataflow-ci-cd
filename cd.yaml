# Cloud Build CD Pipeline for Dataflow Monorepo
# This pipeline deploys multiple Dataflow jobs to Google Cloud
# Triggered manually or on merges to main branch

substitutions:
  _PROJECT_ID: 'your-gcp-project-id'
  _REGION: 'us-central1'
  _TEMP_LOCATION: 'gs://your-bucket/temp'
  _STAGING_LOCATION: 'gs://your-bucket/staging'
  _OUTPUT_LOCATION: 'gs://your-bucket/output'
  _SERVICE_ACCOUNT: 'dataflow-sa@your-gcp-project-id.iam.gserviceaccount.com'
  _NETWORK: 'default'
  _SUBNETWORK: 'regions/us-central1/subnetworks/default'

steps:
  # ==================== Shakespeare WordCount Pipeline ====================
  # Step 1a: Install dependencies for Shakespeare pipeline
  - name: 'python:3.9'
    id: 'install-deps-shakespeare'
    dir: 'pipelines/shakespeare-wordcount'
    entrypoint: 'pip'
    args: ['install', '-r', 'requirements.txt', '--user']

  # Step 2a: Run pre-deployment checks for Shakespeare pipeline
  - name: 'python:3.9'
    id: 'check-shakespeare'
    dir: 'pipelines/shakespeare-wordcount'
    entrypoint: 'python'
    args: ['-m', 'flake8', 'main.py', '--max-line-length=100']
    waitFor: ['install-deps-shakespeare']

  # Step 3a: Deploy Shakespeare pipeline to Dataflow
  - name: 'python:3.9'
    id: 'deploy-shakespeare'
    dir: 'pipelines/shakespeare-wordcount'
    entrypoint: 'python'
    args:
      - 'main.py'
      - '--runner=DataflowRunner'
      - '--project=${_PROJECT_ID}'
      - '--region=${_REGION}'
      - '--temp_location=${_TEMP_LOCATION}/shakespeare'
      - '--staging_location=${_STAGING_LOCATION}/shakespeare'
      - '--output=${_OUTPUT_LOCATION}/shakespeare/wordcount'
      - '--service_account_email=${_SERVICE_ACCOUNT}'
      - '--network=${_NETWORK}'
      - '--subnetwork=${_SUBNETWORK}'
      - '--job_name=shakespeare-wordcount-${SHORT_SHA}'
      - '--setup_file=./setup.py'
      - '--save_main_session'
    waitFor: ['check-shakespeare']
    env:
      - 'GOOGLE_CLOUD_PROJECT=${_PROJECT_ID}'

  # Step 4a: Verify Shakespeare deployment
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'verify-shakespeare'
    entrypoint: 'gcloud'
    args:
      - 'dataflow'
      - 'jobs'
      - 'list'
      - '--region=${_REGION}'
      - '--filter=name:shakespeare-wordcount-${SHORT_SHA}'
      - '--format=table(id,name,type,state)'
    waitFor: ['deploy-shakespeare']

  # ==================== USA Names Stats Pipeline ====================
  # Step 1b: Install dependencies for USA Names pipeline
  - name: 'python:3.9'
    id: 'install-deps-usa-names'
    dir: 'pipelines/usa-names-stats'
    entrypoint: 'pip'
    args: ['install', '-r', 'requirements.txt', '--user']

  # Step 2b: Run pre-deployment checks for USA Names pipeline
  - name: 'python:3.9'
    id: 'check-usa-names'
    dir: 'pipelines/usa-names-stats'
    entrypoint: 'python'
    args: ['-m', 'flake8', 'main.py', '--max-line-length=100']
    waitFor: ['install-deps-usa-names']

  # Step 3b: Deploy USA Names pipeline to Dataflow
  - name: 'python:3.9'
    id: 'deploy-usa-names'
    dir: 'pipelines/usa-names-stats'
    entrypoint: 'python'
    args:
      - 'main.py'
      - '--runner=DataflowRunner'
      - '--project=${_PROJECT_ID}'
      - '--region=${_REGION}'
      - '--temp_location=${_TEMP_LOCATION}/usa-names'
      - '--staging_location=${_STAGING_LOCATION}/usa-names'
      - '--output=${_OUTPUT_LOCATION}/usa-names/stats'
      - '--service_account_email=${_SERVICE_ACCOUNT}'
      - '--network=${_NETWORK}'
      - '--subnetwork=${_SUBNETWORK}'
      - '--job_name=usa-names-stats-${SHORT_SHA}'
      - '--setup_file=./setup.py'
      - '--save_main_session'
    waitFor: ['check-usa-names']
    env:
      - 'GOOGLE_CLOUD_PROJECT=${_PROJECT_ID}'

  # Step 4b: Verify USA Names deployment
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'verify-usa-names'
    entrypoint: 'gcloud'
    args:
      - 'dataflow'
      - 'jobs'
      - 'list'
      - '--region=${_REGION}'
      - '--filter=name:usa-names-stats-${SHORT_SHA}'
      - '--format=table(id,name,type,state)'
    waitFor: ['deploy-usa-names']

# Cloud Build options
options:
  logging: CLOUD_LOGGING_ONLY
  machineType: 'N1_HIGHCPU_8'

# Timeout for the entire build (30 minutes)
timeout: '1800s'

# Available images will be stored in Artifact Registry
images: []
